{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lisbon Airbnb Data\n",
    "\n",
    "## An exploratory analysis\n",
    "\n",
    "Here will be analyzed the data provided regarding Airbnb bookings and property listings. In this analysis the following questions will be addressed:\n",
    "- Who is booking Airbnb rooms in Portugal? (Profiling)\n",
    "- Where do they choose to stay? (Geographic analysis)\n",
    "- When do they do it? (Time Series analysis)\n",
    "\n",
    "All the data was provided by Nova School of Business and Economics' research group: **Data Science for Social Impact and Management**.\n",
    "\n",
    "This analysis is focused on Tourism Management in Portugal and was developed for Turismo de Portugal, a public national institution responsible for promote, enrich and sustain Tourism in the country.\n",
    "\n",
    "The report is structured as follows:\n",
    "\n",
    "\n",
    "- **[Preprocessing and Data Exploration](#Preprocessing-and-Data-Exploration)**\n",
    "    - [Property listings](#Property-listings)\n",
    "    - [Daily Bookings](#Daily-Bookings)\n",
    "    - [Monthly Bookings](#Monthly-Bookings)\n",
    "    - [Listing Reviews](#Listing-Reviews)\n",
    "    - [Outlier Detection](#Outlier-Detection)\n",
    "    - [Missing Values Detection](#Missing-Values-Detection)\n",
    "    - [Correlation Matrices](#Correlation-Matrices)\n",
    "\n",
    "- **[Modelling](#Modelling)**\n",
    "    - [Profiling](#Profiling)\n",
    "\n",
    "- **[Analyst reviews output/Evaluation](#Analyst-reviews-output/Evaluation)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Data Exploration\n",
    "\n",
    "Let's start by importing some necessary libraries and specify tokens and configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from numba import jit\n",
    "import sklearn.cluster as sk_cluster\n",
    "from sklearn import preprocessing, neighbors\n",
    "import fiona\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "# visualizations\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "import plotly.tools as tls # testing function to see wether it allows html to be embedded\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "pd.options.mode.chained_assignment = None # default = 'warn'\n",
    "mapbox_token = 'pk.eyJ1Ijoiam9hb2ZvbnNlY2EiLCJhIjoiY2picXB3cDVvMDczYjJ3bzBxNDV3dGI0MSJ9.XpQDNjTuMAM-xckGln0KrA'\n",
    "\n",
    "preprocess_dir = '_lisbon_data_out/'\n",
    "viz_out = '_lisbon_viz_out/'\n",
    "plt_viz = '_lisbon_plt_viz/'\n",
    "shp_dir = 'districts_shp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = pd.read_csv('../data/Portugal_Property_2018-02-02.csv')\n",
    "daily = pd.read_csv('../data/Portugal_Daily_2018-02-02.csv')\n",
    "monthly = pd.read_csv('../data/Portugal_Monthly_2018-02-02.csv')\n",
    "reviews = pd.read_csv('../data/Portugal_Review_2018-02-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre select area of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Opening shapefile with districts\\' borders...')\n",
    "shp_file = fiona.open(shp_dir+'PRT_adm1.shp')\n",
    "\n",
    "distritos = {0:'Evora', 1:'Aveiro', 2: 'Açores', 3: 'Beja', 4: 'Braga', 5: 'Bragança', 6: 'Castelo Branco',\n",
    "             7: 'Coimbra', 8: 'Faro', 9: 'Guarda', 10: 'Leiria', 11: 'Lisboa', 12: 'Madeira', 13: 'Portalegre',\n",
    "            14: 'Porto', 15: 'Santarem', 16: 'Setubal', 17: 'Viana do Castelo', 18: 'Vila Real', 19: 'Viseu',\n",
    "            30:'unknown' \n",
    "            }\n",
    "\n",
    "print('Formatting shapefile and defining polygons and points...')\n",
    "shp_final = []\n",
    "for distrito in shp_file:\n",
    "    distrito.update({'shape':shape(distrito['geometry'])})\n",
    "    shp_final.append(distrito)\n",
    "\n",
    "def generate_point(row):\n",
    "    return Point(row['Longitude'],row['Latitude'])\n",
    "\n",
    "properties['point'] = properties.apply(generate_point, axis=1)\n",
    "\n",
    "\n",
    "def check_district(row):\n",
    "    for distrito in shp_final:\n",
    "        check = distrito['shape'].contains(row['point'])\n",
    "        if check:\n",
    "            return distrito['id']\n",
    "\n",
    "print('Assigning the respective district id\\'s to the points...')\n",
    "properties['cluster'] = properties.apply(check_district, axis=1)\n",
    "\n",
    "print('Assigning district id\\'s to points outside of borders (i.e., the ones with imprecisions in coordinates)...')\n",
    "properties['cluster'] = properties['cluster'].fillna(30).astype('int')\n",
    "n_neighbors = 3\n",
    "X = properties[properties['cluster'] != 30][['Latitude', 'Longitude']]\n",
    "y = properties[properties['cluster'] != 30]['cluster']\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)\n",
    "knn_prediction = clf.predict(properties[properties['cluster'] == 30][['Latitude', 'Longitude']])\n",
    "properties.loc[properties['cluster'] == 30, 'cluster'] = knn_prediction\n",
    "\n",
    "print('Assigning district names to id\\'s')\n",
    "def get_area_name(value):\n",
    "    return distritos[value]\n",
    "\n",
    "properties['cluster_2'] = properties['cluster'] \n",
    "properties['cluster'] = properties['cluster'].apply(get_area_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Filtering out values outside of Porto...')\n",
    "properties_porto = properties[properties['cluster']=='Porto']\n",
    "property_ids_porto = list(properties_porto['Property ID'])\n",
    "reviews_porto = reviews.loc[reviews['Property ID'].isin(property_ids_porto)]\n",
    "daily_porto = daily.loc[daily['Property ID'].isin(property_ids_porto)]\n",
    "monthly_porto = monthly.loc[monthly['Property ID'].isin(property_ids_porto)]\n",
    "\n",
    "print('Filtering out values outside of Lisbon...')\n",
    "properties_lisbon = properties[properties['cluster']=='Lisboa']\n",
    "property_ids_lisbon = list(properties_lisbon['Property ID'])\n",
    "reviews_lisbon = reviews[reviews['Property ID'].isin(property_ids_lisbon)]\n",
    "daily_lisbon = daily[daily['Property ID'].isin(property_ids_lisbon)]\n",
    "monthly_lisbon = monthly[monthly['Property ID'].isin(property_ids_lisbon)]\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = dict(properties = properties,\n",
    "              daily      = daily,\n",
    "              monthly    = monthly,\n",
    "              reviews    = reviews)\n",
    "\n",
    "for table in ['properties', 'daily', 'monthly', 'reviews']:\n",
    "    print(('Columns in table %s:\\n' % table ), list(tables[table]))\n",
    "    print('Total number of variables in dataset: ', len(list(tables[table])),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# Lisbon\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = properties_lisbon\n",
    "reviews = reviews_lisbon\n",
    "daily = daily_lisbon\n",
    "monthly = monthly_lisbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property listings\n",
    "\n",
    "We'll start by analysing the **Properties table**.\n",
    "It contains thorough information regarding Property listings in the Airbnb platform, from which we will use only a subset of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_df = properties[['Property ID', 'Property Type', 'Listing Type', 'Bedrooms', 'Created Date', 'City',\n",
    "                       'Annual Revenue LTM (USD)', 'Average Daily Rate (Native)', 'Average Daily Rate (USD)', \n",
    "                       'Number of Bookings LTM', 'Max Guests', 'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess how the listings are distributed across Portugal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = go.Data([\n",
    "        go.Scattermapbox(\n",
    "                lat = properties['Latitude'],\n",
    "                lon = properties['Longitude'],\n",
    "                mode='markers',\n",
    "                marker=go.Marker(size= 2),\n",
    "                text = properties['Annual Revenue LTM (USD)']\n",
    "                )\n",
    "        ])\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Location of listings',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_token,\n",
    "        bearing=0,\n",
    "        style='light',\n",
    "        center=dict(\n",
    "            lat=39.64,\n",
    "            lon=-7.95,\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=4.2\n",
    "    ),\n",
    ")\n",
    "\n",
    "wonder_map = go.Figure(data=data, layout=layout)\n",
    "\n",
    "#generate interactive visualization\n",
    "plotly.offline.plot(wonder_map, filename=viz_out+'geographic_analysis_plot.html', show_link=False, auto_open=False)\n",
    "\n",
    "#div_file = plotly.offline.plot(wonder_map, include_plotlyjs=False, output_type='div')\n",
    "\n",
    "#with open('divs/geographic_analysis_plot.html', 'w') as file:\n",
    "#    file.write(div_file)\n",
    "\n",
    "#show plot in notebook:\n",
    "plotly.offline.iplot(wonder_map, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot class variables\n",
    "for column in ['Property Type', 'Listing Type']:\n",
    "        # plot bar chart\n",
    "        props_df[column].value_counts().plot(kind='bar', figsize=(22, 6), title=column)\n",
    "        #plt.set_title(column)\n",
    "        if column == 'City':\n",
    "            plt.savefig(plt_viz+'listing_cities.png')\n",
    "        if column == 'Listing Type':\n",
    "            plt.savefig(plt_viz+'listing_types.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some properties had zero bedrooms, which is clearly a mistake\n",
    "properties['Bedrooms'] = properties['Bedrooms'].replace(to_replace=0.0, value=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Daily Bookings\n",
    "\n",
    "The Daily table contains bookings made in the platform in Portugal on a daily basis. As assessed previously, this is the data available in the table:\n",
    "\n",
    "['Property ID', 'Date', 'Status', 'Booked Date', 'Price (USD)', 'Price (Native)', 'Currency Native', 'Reservation ID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we'll be using:\n",
    "day_df = daily[[ 'Property ID', 'Date', 'Booked Date', 'Price (USD)', 'Reservation ID' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now format the remaining columns to their proper formats and check for extreme values or outliers in the remaining data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df['Booked Date'] = pd.to_datetime(day_df['Booked Date'])\n",
    "day_df['Date'] = pd.to_datetime(day_df['Date'])\n",
    "# Converting Created Date to an integer format to allow plotting\n",
    "day_df['booked_date_format'] = pd.DatetimeIndex(day_df['Booked Date']).astype(np.int64)\n",
    "day_df['date_format'] = pd.DatetimeIndex ( day_df['Date'] ).astype ( np.int64 )\n",
    "\n",
    "print( 'Number of observations in dataset: ', len(day_df['Reservation ID']) )\n",
    "\n",
    "# plotting interval variables (numba.jit is used here to accelerate the processing time)\n",
    "@jit\n",
    "def generate_viz_daily():\n",
    "    column = 'Price (USD)'\n",
    "    # plot histogram\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(x=day_df[column].dropna(), bins=100)\n",
    "    # plot box plot\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.boxplot(x=day_df[column].dropna())\n",
    "    # Plot configurations\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    plt.suptitle(column)\n",
    "    plt.show()\n",
    "    # Computing IQR\n",
    "    Q1 = day_df[column].dropna().quantile(0.25)\n",
    "    Q3 = day_df[column].dropna().quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # Filtering Values between Q1-1.5IQR and Q3+1.5IQR\n",
    "    _filtered = day_df[ Q1-1.5*IQR < day_df[column]]\n",
    "    filtered = _filtered[ _filtered[column] < Q3+1.5*IQR ]\n",
    "    if column == 'date_format':\n",
    "        plt.savefig(plt_viz+'daily_date.png')\n",
    "\n",
    "    if column == 'booked_date_format':\n",
    "        print('Minimum value: ', min(day_df['Booked Date'].dropna()), '   Maximum Value: ', max(day_df['Booked Date'].dropna()))\n",
    "    elif column == 'date_format':\n",
    "        print('Minimum value: ', min(day_df['Date']), '   Maximum Value: ', max(day_df['Date']))\n",
    "    else:\n",
    "        print('Minimum value: ', min(day_df[column]), '   Maximum Value: ', max(day_df[column]))\n",
    "    print('Count of observations outside of Inter Quartile Range: ', len(day_df[column].dropna())-len(filtered[column].dropna()))\n",
    "\n",
    "generate_viz_daily()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(data analysis, refer to outliers, extreme values, distribution, etc)\n",
    "\n",
    "During Clustering: Decide cutoff levels for extreme values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Reviews\n",
    "\n",
    "The Listing Reviews table contains reviews made by users in the platform in Portugal on a monthly basis, as well as some details regarding the user's profile, namely First name, Country of origin, State (if the user is from the USA), City of origin, brief user description, last attended teaching institution and occupation. Again, this is the data available in the table, from which we will select only the variables that are relevant for this analysis:\n",
    "\n",
    "['Property ID', 'Latitude', 'Longitude', 'Address', 'Review Date', 'Review Text', 'User ID', 'Member Since', 'First Name', 'Country', 'State', 'City', 'Description', 'School', 'Work', 'Profile Image URL', 'Profile URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize a very important detail after a first analysis: Data regarding customer profile is not standardized. In order to save space, this first analysis will not be displayed, but rather a final analysis, after preprocessing. So, we will be required to standardize it by parsing each person's country of origin from the input text received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use script developed: reviews_data_standardized.py\n",
    "# This script uses geograpy, a library that helps parsing locations from text.\n",
    "# As we are using python 3.6.3 and geograpy was developed for python 2.7, we will be required to use an adapted version\n",
    "# This version is available in https://github.com/reach2ashish/geograpy, and can be installed by running:\n",
    "# python3 -m pip install git+https://github.com/reach2ashish/geograpy.git\n",
    "\n",
    "from reviews_data_standardized import preprocess_countries_reviews_table\n",
    "\n",
    "# Import output\n",
    "reviews_pre = preprocess_countries_reviews_table(reviews)\n",
    "#reviews_pre = reviews_pre[reviews_pre['Longitude'] != '2013-07-01']\n",
    "reviews_pre.drop_duplicates()\n",
    "reviews_pre['Longitude'].astype('float')\n",
    "print(list(reviews_pre.columns))\n",
    "# Select columns\n",
    "rev_df = reviews_pre[[ 'Property ID', 'User ID', 'Review Date', 'Member Since', 'First Name', \n",
    "                   'Country', 'State', 'City', 'Description', 'School', 'Work', \n",
    "                   'country_after_parse', 'country_from_city_parse', 'final_country_parse' ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is the result of our preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot class variables\n",
    "for column in ['country_after_parse', 'country_from_city_parse', 'final_country_parse']: #'Description', 'School', 'Work']:\n",
    "    # plot bar chart\n",
    "    rev_df[rev_df[column] != '-'][column].value_counts().nlargest(50).plot(kind='bar', figsize=(22, 6), title=column)\n",
    "    #plt.set_title(column)\n",
    "    if column == 'final_country_parse':\n",
    "        plt.savefig(plt_viz+'final_country_parse.png')\n",
    "    plt.show()\n",
    "    print( 'Count of non-parsed origins: ', len(rev_df[rev_df[column] == '-'][column]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the final parse with the initial top 50 origins distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot bar chart\n",
    "reviews[reviews['Country'] != '-']['Country'].value_counts().nlargest(50).plot(kind='bar', figsize=(22, 6), title='Country')\n",
    "#plt.set_title(column)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finally, a sum up of all the analysis in all tables: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "table_name = ['properties', 'daily', 'monthly', 'reviews']\n",
    "for table in [properties, daily, monthly, reviews_pre]:\n",
    "    print(table_name[i])\n",
    "    i+=1\n",
    "    analysis = table.describe().T\n",
    "    analysis['na_count_percent'] = np.round(table.isnull().sum() / len(table) *100, 1)\n",
    "    analysis['na_count_percent'] = analysis['na_count_percent'].astype('str')+'%'\n",
    "    analysis = analysis.reset_index()\n",
    "    columns = list(analysis.columns)\n",
    "    columns[0] = 'columns'\n",
    "    analysis.columns = columns\n",
    "    \n",
    "    trace = go.Table(\n",
    "        header=dict(values=analysis.columns, fill = dict(color='#C2D4FF'), align = ['center'] * 5),\n",
    "        cells=dict(values=[analysis[column] for column in analysis.columns],fill = dict(color='#F5F8FF'), \n",
    "                   align = ['left'] * 5))\n",
    "    plotly.offline.iplot([trace], show_link=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Detection\n",
    "\n",
    "In order to proceed to the next step of the analysis, we will need to handle missing values. As such, we will check for inexistent values in all variables from the 3 different tables, displayed in the previous set of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nan_props = properties.isnull().sum()\n",
    "nan_daily = daily.isnull().sum()\n",
    "nan_monthly = monthly.isnull().sum()\n",
    "nan_reviews = reviews.isnull().sum()\n",
    "\n",
    "# create dict with NaN table counts for loop\n",
    "nans = dict(nan_props = nan_props,\n",
    "            nan_daily = nan_daily,\n",
    "            nan_monthly = nan_monthly,\n",
    "            nan_reviews = nan_reviews)\n",
    "\n",
    "# set dict with total size of data sets\n",
    "lengths = dict(nan_props      = len(properties),\n",
    "               nan_daily      = len(daily),\n",
    "               nan_monthly    = len(monthly),\n",
    "               nan_reviews    = len(reviews))\n",
    "\n",
    "nan_tables = []\n",
    "for nan_count in nans:\n",
    "    nan_counts = pd.DataFrame(nans[nan_count], columns= [ 'nan_count' ]).reset_index()\n",
    "    nan_counts.columns = ['columns','nan_count']\n",
    "    # calculate percentage over the total size of the data set for relative analysis\n",
    "    nan_counts['nan%'] = nan_counts['nan_count'].apply(lambda x: (x/lengths[nan_count])*100)\n",
    "    nan_tables.append(nan_counts)\n",
    "\n",
    "# configure table\n",
    "final_nan = pd.concat(nan_tables, ignore_index=True, axis=1)\n",
    "final_nan.columns = ['Properties', 'nan_p', 'nan%_p', 'Daily', 'nan_d', 'nan%_d', \n",
    "                     'Monthly', 'nan_m', 'nan%_m', 'Reviews', 'nan_r', 'nan%_r']\n",
    "\n",
    "# replace NaN's where there are no columns to show\n",
    "final_nan = final_nan.fillna(value='-')\n",
    "\n",
    "# configure table viz\n",
    "trace = go.Table(\n",
    "    header=dict(values=final_nan.columns, fill = dict(color='#C2D4FF'), align = ['left'] * 5),\n",
    "    cells=dict(values=[final_nan[column] for column in final_nan.columns],fill = dict(color='#F5F8FF'), \n",
    "               align = ['left'] * 5))\n",
    "plotly.offline.iplot([trace], show_link=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that there are some columns that have a very high percentage of missing values, which we must consider whether to use them or not and how can we proceed to fill the missing values in the variables we will be using. To do this, we must consider each variable's importance to our goal: **Tourism profiling and flows analysis**.\n",
    "\n",
    "Once again, given the size of the present datasets, we will use the median to fill the missing values in the necessary columns, i.e., the ones we will be using for the remainder of the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# outliers will be handled in these tables\n",
    "tables_filtered2 = dict(properties = properties,\n",
    "                       daily      = daily,\n",
    "                       monthly    = monthly,\n",
    "                       reviews    = reviews_pre)\n",
    "\n",
    "tables_filtered2['monthly']['Revenue (USD)'] = tables_filtered2['monthly']['Revenue (USD)'].fillna(value=0)\n",
    "\n",
    "medians_props = tables_filtered2['properties'].median(skipna=True)\n",
    "medians_monthly = tables_filtered2['monthly'].median(skipna=True)\n",
    "medians_daily = tables_filtered2['daily'].median(skipna=True)\n",
    "medians_reviews = tables_filtered2['reviews'].median(skipna=True)\n",
    "tables_filtered2['properties'] = tables_filtered2['properties'].fillna(value=medians_props)\n",
    "tables_filtered2['monthly'] = tables_filtered2['monthly'].fillna(value=medians_monthly)\n",
    "tables_filtered2['daily'] = tables_filtered2['daily'].fillna(value=medians_daily)\n",
    "tables_filtered2['reviews'] = tables_filtered2['reviews'].fillna(value=medians_reviews)\n",
    "\n",
    "tables_filtered = tables_filtered2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"the stats are inaccurate. The number of blocked days is wrong. The number of booked dates is wrong. The number of unbooked days is wrong. Clearly, AirBNB is not doing their own calculations correctly. Thankfully, the SuperHost numbers are correct.\"**\n",
    " \n",
    "It is impossible to assess whether the number of reviews is representative, given that Airbnb's accuracy in their data is not certain. Furthermore, Airbnb's reviews can be either public or private. As we are only using publicly available data, we do not have access to user data that left a private review, or no review at all. So, we are analysing user profiles that represent 10% of the total bookings that were actually completed. Although the user data sample extracted from the overall reviews was not randomly generated, it is highly representative. The number of total completed bookings made between September 1st 2014 and December 31st 2017 is 11.550 million Bookings which implies a minimum sample size of 16564 for a 99% confidence level and 1% margin of error. As our sample has a size of 1.2 million observations, it is statistically significant (although, we cannot conclude that it is an unbiased sample, as it was not randomly selected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "Aside from the excessively high revenue values in the table Property Listings, we did not detect any other value that we were certain to be wrong. So, we conclude that our dataset doesn't have values that require its removal for this reason. Although, we did find some outliers.\n",
    "\n",
    "In order to maintain a simple, straightforward analysis, we will use the Interquartile Range method (IQR) to remove these extreme values:\n",
    "\n",
    "1) **IQR = Q3 – Q1**\n",
    "\n",
    "2) **If a data point is below Q1 – 1.5×IQR or above Q3 + 1.5×IQR, it is viewed as being too far from the central values to be reasonable.**\n",
    "\n",
    "After this, we will have two options: Remove these observations, or set these values as missing and reapply the method used in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#tables_filtered['properties'] = tables_filtered['properties'][tables_filtered['properties']['Number of Bookings LTM']>5]\n",
    "\n",
    "# Filtering Values between Q1-1.5IQR and Q3+1.5IQR\n",
    "for table in list(tables_filtered.keys()):\n",
    "    if table == 'daily':\n",
    "        #print('      ',table)\n",
    "        for column in list(tables_filtered[table].columns):\n",
    "            if column in ['Price (USD)']:#'Available Days']:\n",
    "                #print(column)\n",
    "                #Q1 = tables_filtered[table][column].dropna().quantile(0.25)\n",
    "                #Q3 = tables_filtered[table][column].dropna().quantile(0.75)\n",
    "                #IQR = Q3 - Q1\n",
    "                #_filtered = tables_filtered[table][ Q1-1.5*IQR < tables_filtered[table][column]]\n",
    "                #filtered = _filtered[ _filtered[column] < Q3+1.5*IQR ]\n",
    "                #tables_filtered[table] = filtered\n",
    "                #print(len(tables_filtered[table]))\n",
    "                tables_filtered[table] = tables_filtered[table][tables_filtered[table][column] < 400]\n",
    "                \n",
    "    elif table == 'monthly':\n",
    "        column = 'ADR (USD)'\n",
    "        condition = tables_filtered[table][column] < 2000.00\n",
    "        tables_filtered[table] = tables_filtered[table][condition]\n",
    "    \n",
    "    elif table == 'properties':\n",
    "        tables_filtered[table] = tables_filtered[table][tables_filtered[table]['Annual Revenue LTM (USD)'] < 100000]\n",
    "    \n",
    "    elif table == 'reviews':\n",
    "        tables_filtered[table] = tables_filtered[table].drop_duplicates()\n",
    "\n",
    "#tbls = list(tables_filtered.keys())\n",
    "\n",
    "print('Number of observations filtered:')\n",
    "print('properties',': ', (len(tables['properties'])-len(tables_filtered['properties'])))\n",
    "print('daily',': ', (len(tables['daily'])-len(tables_filtered['daily'])))\n",
    "print('monthly',': ', (len(tables['monthly'])-len(tables_filtered['monthly'])))\n",
    "print('reviews',': ', (len(tables['reviews'])-len(tables_filtered['reviews'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "To analyse patterns in the provided data we will extract as many relevant variables from customers as possible. Additionally, we will also segment Property Listings by geographic location and associate them to the customers.\n",
    "\n",
    "### Property Listings Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_column_list = [column for column in tables_filtered['properties'].columns]\n",
    "properties_column_list.remove('Zipcode')\n",
    "properties_column_list.remove('Metropolitan Statistical Area')\n",
    "properties_column_list.remove('Property ID')\n",
    "properties_column_list.remove('Host ID')\n",
    "properties_column_list.remove('Last Scraped Date')\n",
    "properties_column_list.remove('Country')\n",
    "properties_column_list.remove('State')\n",
    "properties_column_list.remove('City')\n",
    "properties_column_list.remove('Neighborhood')\n",
    "properties_column_list.remove('Checkout Time')\n",
    "properties_column_list.remove('Superhost')\n",
    "properties_column_list.remove('Currency Native')\n",
    "properties_column_list.remove('Listing Type')\n",
    "properties_column_list.remove('Listing Title')\n",
    "properties_column_list.remove('Property Type')\n",
    "properties_column_list.remove('Cancellation Policy')\n",
    "properties_column_list.remove('Calendar Last Updated')\n",
    "properties_column_list.remove('Listing URL')\n",
    "properties_column_list.remove('Business Ready')\n",
    "properties_column_list.remove('Listing Main Image URL')\n",
    "properties_column_list.remove('Longitude')\n",
    "properties_column_list.remove('Latitude')\n",
    "\n",
    "corr_matrix = tables_filtered['properties'][properties_column_list].reset_index().corr(method='pearson')\n",
    "\n",
    "\n",
    "trace = go.Heatmap( z= corr_matrix.values.tolist(),\n",
    "                    x= properties_column_list,\n",
    "                    y= properties_column_list,\n",
    "                  )\n",
    "data = go.Data([trace])\n",
    "\n",
    "layout = dict( margin = dict(t=50,r=50,b=150,l=150))\n",
    "\n",
    "figure = dict(data=data , layout=layout )\n",
    "\n",
    "plotly.offline.plot(figure, filename=viz_out+'correlation_matrix_test.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(figure, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Insights taken from the correlation matrix goes here, and there are plenty)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Listings Geographic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Opening shapefile with districts\\' borders...')\n",
    "\n",
    "shp_file = fiona.open(shp_dir+'PRT_adm2.shp')\n",
    "\n",
    "distritos = pd.read_csv(shp_dir+'PRT_adm2.csv')[['ID_2', 'NAME_2']]\n",
    "distritos = distritos.set_index('ID_2')\n",
    "\n",
    "print('Formatting shapefile and defining polygons and points...')\n",
    "shp_final = []\n",
    "for distrito in shp_file:\n",
    "    distrito.update({'shape':shape(distrito['geometry'])})\n",
    "    shp_final.append(distrito)\n",
    "\n",
    "def generate_point(row):\n",
    "    return Point(row['Longitude'],row['Latitude'])\n",
    "\n",
    "tables_filtered['properties']['point'] = tables_filtered['properties'].apply(generate_point, axis=1)\n",
    "\n",
    "\n",
    "def check_district(row):\n",
    "    for distrito in shp_final:\n",
    "        check = distrito['shape'].contains(row['point'])\n",
    "        if check:\n",
    "            return distrito['id']\n",
    "\n",
    "print('Assigning the respective district id\\'s to the points...')\n",
    "tables_filtered['properties']['cluster'] = tables_filtered['properties'].apply(check_district, axis=1)\n",
    "\n",
    "print('Assigning district id\\'s to points outside of borders (i.e., the ones with imprecisions in coordinates)...')\n",
    "tables_filtered['properties']['cluster'] = tables_filtered['properties']['cluster'].fillna(30).astype('int')\n",
    "n_neighbors = 3\n",
    "X = tables_filtered['properties'][tables_filtered['properties']['cluster'] != 30][['Latitude', 'Longitude']]\n",
    "y = tables_filtered['properties'][tables_filtered['properties']['cluster'] != 30]['cluster']\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)\n",
    "knn_prediction = clf.predict(tables_filtered['properties'][tables_filtered['properties']['cluster'] == 30][['Latitude', 'Longitude']])\n",
    "tables_filtered['properties'].loc[tables_filtered['properties']['cluster'] == 30, 'cluster'] = knn_prediction\n",
    "\n",
    "print('Assigning district names to id\\'s')\n",
    "def get_area_name(value):\n",
    "    return distritos.loc[value+1]['NAME_2']\n",
    "\n",
    "\n",
    "tables_filtered['properties']['cluster_2'] = tables_filtered['properties']['cluster']\n",
    "tables_filtered['properties']['cluster'] = tables_filtered['properties']['cluster'].apply(get_area_name)\n",
    "\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tables_filtered['properties'] = tables_filtered['properties'].sort_values('cluster')\n",
    "map_data = tables_filtered['properties'][tables_filtered['properties']['Number of Bookings LTM']>5]\n",
    "\n",
    "data = go.Data([\n",
    "        go.Scattermapbox(\n",
    "                lat = map_data['Latitude'],\n",
    "                lon = map_data['Longitude'],\n",
    "                mode='markers',\n",
    "                marker=go.Marker(size= 2,\n",
    "                                 color=map_data['cluster_2'] ),\n",
    "                text = map_data['cluster']\n",
    "                )\n",
    "        ])\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Location of listings',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_token,\n",
    "        bearing=0,\n",
    "        style='dark',\n",
    "        center=dict(\n",
    "            lat=39.64,\n",
    "            lon=-7.95,\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=4.2\n",
    "    ),\n",
    ")\n",
    "\n",
    "wonder_map = go.Figure(data=data, layout=layout)\n",
    "\n",
    "#generate interactive visualization\n",
    "plotly.offline.plot(wonder_map, filename=viz_out+'geographic_clustering.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(wonder_map, show_link=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Listings' Value Clustering\n",
    "\n",
    "In this clustering analysis we will cluster listings by economic and property features, as well as details in Airbnb platform such as the number of pictures in a listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables\n",
    "filter_condition = tables_filtered['properties']['Number of Bookings LTM'] > 4\n",
    "val = tables_filtered['properties'][filter_condition]\\\n",
    "                                   [['Property ID', \n",
    "                                     'Latitude', \n",
    "                                     'Longitude', \n",
    "                                     'Annual Revenue LTM (USD)', \n",
    "                                     'Average Daily Rate (USD)', \n",
    "                                     'Occupancy Rate LTM', \n",
    "                                     'Number of Bookings LTM',\n",
    "                                     # 'Number of Reviews', <- highly correlated with number of bookings\n",
    "                                     'Bedrooms', \n",
    "                                     'Bathrooms', \n",
    "                                     'Max Guests']]\n",
    "\n",
    "val = val.fillna(val.mean())\n",
    "\n",
    "# Normalization of the variables we will use for the value clustering\n",
    "vars_ = ['Annual Revenue LTM (USD)', 'Average Daily Rate (USD)', 'Occupancy Rate LTM', \n",
    "                     'Number of Bookings LTM']\n",
    "val[vars_] = preprocessing.scale(val[vars_])\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "inertia_table = pd.DataFrame(index=list(range(2,21)))\n",
    "\n",
    "# estimating optimal number of clusters\n",
    "for nmbr_of_runs in list(range(1,4)):\n",
    "    \n",
    "    distances = []\n",
    "    for nmbr in list(range(2,21)):\n",
    "        # Define K-means algorithm\n",
    "        km = sk_cluster.KMeans(n_clusters=nmbr, random_state=None)\n",
    "        km.fit( val[vars_] )\n",
    "        # Get Cluster assignment Labels\n",
    "        ine = km.inertia_\n",
    "        distances.append(ine)\n",
    "        \n",
    "    inertia_table['inertia_iteration_'+str(nmbr_of_runs)] = distances\n",
    "\n",
    "inertia_table.T.mean().plot.line()\n",
    "plt.show()\n",
    "\n",
    "#inertia_table.plot.line()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define K-means algorithm\n",
    "km = sk_cluster.KMeans(n_clusters=5, random_state=None)\n",
    "km.fit(val[['Annual Revenue LTM (USD)', 'Average Daily Rate (USD)', 'Occupancy Rate LTM', 'Number of Bookings LTM']])\n",
    "\n",
    "\n",
    "\n",
    "# Get Cluster assignment Labels\n",
    "labels = km.labels_\n",
    "val['val_cluster'] = labels\n",
    "\n",
    "data = go.Data([\n",
    "        go.Scattermapbox(\n",
    "                lat = val['Latitude'],\n",
    "                lon = val['Longitude'],\n",
    "                mode='markers',\n",
    "                marker=go.Marker(size= 2,\n",
    "                                 color=val['val_cluster'] ),\n",
    "                text = val['val_cluster']\n",
    "                )\n",
    "        ])\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Location of listings',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_token,\n",
    "        bearing=0,\n",
    "        style='dark',\n",
    "        center=dict(\n",
    "            lat=39.64,\n",
    "            lon=-7.95,\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=4.2\n",
    "    ),\n",
    ")\n",
    "\n",
    "super_map = go.Figure(data=data, layout=layout)\n",
    "\n",
    "#generate interactive visualization\n",
    "plotly.offline.plot(super_map, filename=viz_out+'value_clustering.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(super_map, show_link=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to merge and adapt our data (group it by value cluster, region, month, etc) in order to analyse it further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the tables we will use\n",
    "rev = tables_filtered['reviews'][['User ID', 'Property ID', 'final_country_parse', 'Review Date']]\n",
    "prop = tables_filtered['properties'][[ 'Property ID', 'Average Daily Rate (USD)', 'Listing Type', 'Bedrooms', 'cluster' ]]\n",
    "day = tables_filtered['daily'][tables_filtered['daily']['Status'] == 'R'][['Property ID', 'Booked Date', 'Date', 'Price (USD)']]\n",
    "mon = tables_filtered['monthly'][[ 'Property ID', 'Reporting Month', 'Occupancy Rate', 'ADR (USD)', 'Number of Reservations', 'Revenue (USD)', 'Reservation Days' ]]\n",
    "\n",
    "# Merge tables into 2\n",
    "tourists = pd.merge(rev, prop, how= 'left', on= 'Property ID')\n",
    "locations = pd.merge(mon, prop, how = 'left', on= 'Property ID')\n",
    "\n",
    "# Format columns\n",
    "tourists['Review Date'] = pd.to_datetime(tourists['Review Date'])\n",
    "locations['Reporting Month'] = pd.to_datetime(locations['Reporting Month'])\n",
    "locations['Review Date'] = locations['Reporting Month']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tourists['Listing Type'] = tourists['Listing Type'].fillna('unknown')\n",
    "\n",
    "\n",
    "# define class variables by 1 or 0's for each column to determine percentages afterwards\n",
    "def get_cluster(value, cluster):\n",
    "    if value == cluster:\n",
    "        ones = 1\n",
    "    else:\n",
    "        ones = 0\n",
    "    return ones\n",
    "\n",
    "# apply function to the class type columns\n",
    "cluster_nums = list(tourists.sort_values('cluster')['cluster'].dropna().unique())\n",
    "#cluster_nums.append(1000)\n",
    "cluster_nums_columns = ['cluster_'+str(cluster) for cluster in cluster_nums]\n",
    "for cluster in cluster_nums:\n",
    "    column_name = 'cluster_'+str(cluster)\n",
    "    tourists[column_name] = tourists['cluster'].apply(lambda x: get_cluster(x, cluster))\n",
    "\n",
    "# apply function to the class type columns\n",
    "property_type_index = list(tourists['Listing Type'].unique())\n",
    "property_type_columns = [ 'listing_type_'+str(listing) for listing in property_type_index]\n",
    "for listing_type in property_type_index:\n",
    "    column_name = 'listing_type_'+str(listing_type)\n",
    "    tourists[column_name] = tourists['Listing Type'].apply(lambda x: get_cluster(x, listing_type))\n",
    "\n",
    "# apply function to the class type columns\n",
    "origins_index = list(tourists['final_country_parse'].value_counts().nlargest(30).keys())\n",
    "#origins_index.remove('French Southern Territories')\n",
    "#origins_index.remove('Saint Helena, Ascension and Tristan da Cunha')\n",
    "#origins_index.remove('Lao People\\'s Democratic Republic')\n",
    "for origin in origins_index:\n",
    "    tourists[origin] = tourists['final_country_parse'].apply(lambda x: get_cluster(x, origin))\n",
    "\n",
    "\n",
    "# generate final tables for visualization\n",
    "origins_grouped = tourists.drop(origins_index, axis=1).groupby(['final_country_parse', 'Review Date']).mean()\n",
    "origins_grouped['review_count'] = tourists.groupby(['final_country_parse', 'Review Date']).size()\n",
    "\n",
    "regions_grouped = tourists.drop(cluster_nums_columns, axis=1).groupby(['cluster', 'Review Date']).mean()\n",
    "regions_grouped['review_count'] = tourists.groupby(['cluster', 'Review Date']).size()\n",
    "regions_grouped = regions_grouped.reset_index()\n",
    "\n",
    "pre_merg = locations[[ 'cluster', 'Review Date', 'Occupancy Rate', 'Number of Reservations', 'Revenue (USD)', 'Bedrooms' ]]\n",
    "pre_merg1 = pre_merg.groupby(['cluster', 'Review Date']).mean()\n",
    "pre_merg2 = pre_merg.groupby(['cluster', 'Review Date']).sum()\n",
    "pre_merg3 = pre_merg1[[ 'Occupancy Rate' ]]\n",
    "pre_merg3['Sum of Reservations'] = pre_merg2['Number of Reservations']\n",
    "pre_merg3['projected_revenue'] = pre_merg2['Revenue (USD)']\n",
    "pre_merg3['Avg Occupancy Rate'] = pre_merg3['Occupancy Rate']\n",
    "final_merg = pre_merg3[[ 'Sum of Reservations', 'Avg Occupancy Rate', 'projected_revenue' ]].reset_index()\n",
    "\n",
    "regions_grouped = pd.merge(regions_grouped, final_merg, on=['cluster', 'Review Date'], how='left')\n",
    "\n",
    "# overall instead of regions\n",
    "overall_grouped = tourists.drop(cluster_nums_columns, axis=1).groupby(['Review Date']).mean()\n",
    "overall_grouped['review_count'] = tourists.groupby(['Review Date']).size()\n",
    "overall_grouped = overall_grouped.reset_index()\n",
    "\n",
    "ovrl_pre_merg = locations[['Review Date', 'Occupancy Rate', 'Number of Reservations', 'Revenue (USD)' ]]\n",
    "ovrl_pre_merg1 = ovrl_pre_merg.groupby(['Review Date']).mean()\n",
    "ovrl_pre_merg2 = ovrl_pre_merg.groupby(['Review Date']).sum()\n",
    "ovrl_pre_merg3 = ovrl_pre_merg1[[ 'Occupancy Rate' ]]\n",
    "ovrl_pre_merg3['Sum of Reservations'] = ovrl_pre_merg2['Number of Reservations']\n",
    "ovrl_pre_merg3['projected_revenue'] = ovrl_pre_merg2['Revenue (USD)']\n",
    "ovrl_pre_merg3['Avg Occupancy Rate'] = ovrl_pre_merg3['Occupancy Rate']\n",
    "ovrl_final_merg = ovrl_pre_merg3[[ 'Sum of Reservations', 'Avg Occupancy Rate', 'projected_revenue' ]].reset_index()\n",
    "\n",
    "overall_grouped = pd.merge(overall_grouped, ovrl_final_merg, on=['Review Date'], how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# export tables\n",
    "regions_grouped.to_csv(preprocess_dir+'portuguese_regions_clusters.csv')\n",
    "origins_grouped.to_csv(preprocess_dir+'countries_of_origin.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": regions_grouped[regions_grouped['cluster'] == cluster]['Review Date'], \n",
    "        \"y\": regions_grouped[regions_grouped['cluster'] == cluster]['Sum of Reservations'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Number of reservations per cluster\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Sum of Reservations\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'reservation_growth_per_region.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": regions_grouped[regions_grouped['cluster'] == cluster]['Review Date'], \n",
    "        \"y\": regions_grouped[regions_grouped['cluster'] == cluster]['projected_revenue'], \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Revenue per cluster\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Sum of Revenue\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'revenue_growth_per_region.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_list = []\n",
    "\n",
    "for cluster in cluster_nums:\n",
    "    trace = {\n",
    "        \"x\": regions_grouped[regions_grouped['cluster'] == cluster]['Review Date'], \n",
    "        \"y\": regions_grouped[regions_grouped['cluster'] == cluster]['projected_revenue'] / regions_grouped[regions_grouped['cluster'] == cluster]['Sum of Reservations']\n",
    "        , \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Revenue per reservation\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2016-03-30 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [0, 1000], \n",
    "    \"title\": \"Revenue/Reservations Ratio\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'revenue_per_reservation_per_region.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prop2 = tables_filtered['properties'][tables_filtered['properties']['Number of Bookings LTM']>4][[ 'Property ID', 'Average Daily Rate (USD)', 'Listing Type', 'Bedrooms', 'Created Date', 'cluster']]\n",
    "\n",
    "daterange = pd.date_range('2009-10-01','2018-01-01' , freq='1M') \n",
    "daterange = daterange.union([daterange[-1] + 1])  \n",
    "daterange = [date.replace(day=1).strftime('%Y-%m-%d') for date in daterange]\n",
    "\n",
    "\n",
    "df_list = []\n",
    "for cluster in cluster_nums:\n",
    "    df1 = pd.DataFrame(daterange)\n",
    "    df1['date'] = df1[0]\n",
    "    df1['cluster'] = cluster\n",
    "    df_list.append(df1[['cluster','date']])\n",
    "\n",
    "indexed = df_list[0]\n",
    "for df__ in list(range(1,len(df_list))):\n",
    "    indexed = indexed.append(df_list[df__], ignore_index=True)\n",
    "\n",
    "\n",
    "df_for_calcs = prop2[['Created Date', 'cluster']].dropna()\n",
    "df_for_calcs['Bedrooms'] = prop2['Bedrooms']\n",
    "indexed['date'] = pd.to_datetime(indexed['date'])\n",
    "df_for_calcs['date'] = pd.to_datetime(df_for_calcs['Created Date'])\n",
    "\n",
    "indexed['bedroom_count'] = 0\n",
    "indexed['property_count'] = 0\n",
    "for row in range(len(indexed)):\n",
    "    cluster = indexed['cluster'][row]\n",
    "    current_time = indexed['date'][row]\n",
    "    cluster_filter = df_for_calcs[df_for_calcs['cluster']==cluster]\n",
    "    time_filter = cluster_filter[cluster_filter['date'] < current_time]\n",
    "    indexed['property_count'][row] = len(time_filter)\n",
    "    indexed['bedroom_count'][row] = time_filter.sum()['Bedrooms']\n",
    "\n",
    "\n",
    "supply_demand_table = regions_grouped[['cluster', 'Review Date', 'Sum of Reservations', 'projected_revenue']]\n",
    "supply_demand_table['date'] = supply_demand_table['Review Date']\n",
    "supply_demand_table[['cluster', 'date', 'Sum of Reservations', 'projected_revenue']]\n",
    "\n",
    "supply_demand_table = pd.merge(supply_demand_table, indexed, on=['cluster', 'date'], how='left')\n",
    "\n",
    "supply_demand_table.dropna()\n",
    "\n",
    "\n",
    "# plot viz\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:\n",
    "    table_filtered__ = supply_demand_table[supply_demand_table['cluster'] == cluster]\n",
    "    trace = {\n",
    "        \"x\": table_filtered__['date'], \n",
    "        \"y\": table_filtered__['Sum of Reservations'] / table_filtered__['property_count']\n",
    "        , \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Demand/Supply Ratio\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Reservations/Properties Ratio\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'demand_supply_plot.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pies = []\n",
    "domains = [\n",
    "    {'y': [0.8, 1-0.03], 'x': [0, 0.25-0.02]},\n",
    "    {'y': [0.8, 1-0.03], 'x': [0.25, 0.5-0.02]},\n",
    "    {'y': [0.8, 1-0.03], 'x': [0.5, 0.75-0.02]},\n",
    "    {'y': [0.8, 1-0.03], 'x': [0.75, 1-0.02]},\n",
    "    {'y': [0.6, 0.8-0.03], 'x': [0, 0.25-0.02]},\n",
    "    {'y': [0.6, 0.8-0.03], 'x': [0.25, 0.5-0.02]},\n",
    "    {'y': [0.6, 0.8-0.03], 'x': [0.5, 0.75-0.02]},\n",
    "    {'y': [0.6, 0.8-0.03], 'x': [0.75, 1-0.02]},\n",
    "    {'y': [0.4, 0.6-0.03], 'x': [0, 0.25-0.02]},\n",
    "    {'y': [0.4, 0.6-0.03], 'x': [0.25, 0.5-0.02]},\n",
    "    {'y': [0.4, 0.6-0.03], 'x': [0.5, 0.75-0.02]},\n",
    "    {'y': [0.4, 0.6-0.03], 'x': [0.75, 1-0.02]},\n",
    "    {'y': [0.2, 0.4-0.03], 'x': [0, 0.25-0.02]},\n",
    "    {'y': [0.2, 0.4-0.03], 'x': [0.25, 0.5-0.02]},\n",
    "    {'y': [0.2, 0.4-0.03], 'x': [0.5, 0.75-0.02]},\n",
    "    {'y': [0.2, 0.4-0.03], 'x': [0.75, 1-0.02]},\n",
    "    {'y': [0, 0.2-0.03], 'x': [0, 0.25-0.02]},\n",
    "    {'y': [0, 0.2-0.03], 'x': [0.25, 0.5-0.02]},\n",
    "    {'y': [0, 0.2-0.03], 'x': [0.5, 0.75-0.02]},\n",
    "    {'y': [0, 0.2-0.03], 'x': [0.75, 1-0.02]},\n",
    "    ]\n",
    "\n",
    "x_coordinates_ = [0.115, 0.25+0.115, 0.5+0.115, 0.75+0.115]\n",
    "y_coordinates_ = [0.99, 0.79, 0.58, 0.385, 0.17]\n",
    "x_coordinates = x_coordinates_\n",
    "y_coordinates = []\n",
    "for i in range(0,4):\n",
    "    x_coordinates = x_coordinates + x_coordinates_\n",
    "    y_coordinates = y_coordinates + y_coordinates_\n",
    "\n",
    "y_coordinates = sorted(y_coordinates, reverse=True)\n",
    "\n",
    "annotations_coords = []\n",
    "i=0\n",
    "for cluster in cluster_nums:\n",
    "    origins_per_cluster = tourists[['cluster', 'Review Date', 'final_country_parse']]\n",
    "    filter_ = origins_per_cluster['cluster'] == cluster\n",
    "    filter2 = origins_per_cluster['final_country_parse'] != '-'\n",
    "    vals = origins_per_cluster[filter_][filter2][['final_country_parse', 'cluster']].groupby('final_country_parse').count()\n",
    "\n",
    "    other_count = vals['cluster'].sum() - vals.nlargest(15, 'cluster')['cluster'].sum()\n",
    "\n",
    "    pie_data = vals.nlargest(10, 'cluster' )\n",
    "    pie_data.loc['others'] = [other_count]\n",
    "\n",
    "    labels = list(pie_data.index)\n",
    "    values = list(pie_data['cluster'])\n",
    "    name = cluster\n",
    "    \n",
    "    a_nice_pie = go.Pie(values=values, \n",
    "                        labels=labels, \n",
    "                        name=name,\n",
    "                        domain=domains[i],\n",
    "                        textinfo=\"none\",\n",
    "#                        hole = .4\n",
    "                       )\n",
    "    \n",
    "    pies.append(a_nice_pie)\n",
    "\n",
    "    annotations=go.Annotation(\n",
    "                x=x_coordinates[i],\n",
    "                y=y_coordinates[i],\n",
    "                showarrow=False,\n",
    "                xanchor='center',\n",
    "#                xref='paper',\n",
    "#                yref='paper',\n",
    "                align='center',\n",
    "                text=cluster,\n",
    "#                ax=0,\n",
    "#                ay=-40\n",
    "                )\n",
    "    annotations_coords.append(annotations)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "layout = go.Layout(autosize = True,\n",
    "                   showlegend=True,\n",
    "                   #width=600,\n",
    "                   height=1000,\n",
    "                   annotations=go.Annotations(annotations_coords),\n",
    "                   title = 'Nationality Representation For Each Region')\n",
    "\n",
    "fig = go.Figure(data = pies, layout = layout)\n",
    "\n",
    "plotly.offline.plot(fig, filename=viz_out+'pies_nacionality_per_region.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "origins_grouped_ovrl = tourists.drop(origins_index, axis=1).groupby(['final_country_parse']).mean()\n",
    "origins_grouped_ovrl['review_count'] = tourists.groupby(['final_country_parse']).size()\n",
    "\n",
    "origins_grouped_ovrl.columns\n",
    "\n",
    "origins_grouped_ovrl = origins_grouped_ovrl[[ 'review_count', 'Average Daily Rate (USD)', 'Bedrooms',# 'cluster_Aveiro',\n",
    "                                              #'cluster_Açores', 'cluster_Beja', 'cluster_Braga', 'cluster_Bragança',\n",
    "                                              #'cluster_Castelo Branco', 'cluster_Coimbra', 'cluster_Evora',\n",
    "                                              #'cluster_Faro', 'cluster_Guarda', 'cluster_Leiria', 'cluster_Lisboa',\n",
    "                                              #'cluster_Madeira', 'cluster_Portalegre', 'cluster_Porto',\n",
    "                                              #'cluster_Santarem', 'cluster_Setubal', 'cluster_Viana do Castelo',\n",
    "                                              #'cluster_Vila Real', 'cluster_Viseu',\n",
    "                                              'listing_type_Entire home/apt', 'listing_type_Private room',\n",
    "                                              'listing_type_unknown', 'listing_type_Shared room' ]]\n",
    "\n",
    "filter_ = origins_grouped_ovrl['review_count'] > 1000\n",
    "origins_grouped_ovrl = origins_grouped_ovrl[filter_]\n",
    "\"\"\"\n",
    "origins_grouped_ovrl['most_visited_cluster'] = '-'\n",
    "origins_grouped_ovrl['second_most_visited'] = '-'\n",
    "origins_grouped_ovrl['third_most_visited'] = '-'\n",
    "for row in range(len(origins_grouped_ovrl)):\n",
    "    highest_value = 0\n",
    "    second_highest_value = 0\n",
    "    third_highest_value = 0\n",
    "    \n",
    "    highest_i = 0\n",
    "    second_highest_i = 0\n",
    "    third_highest_i = 0\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        current_value = origins_grouped_ovrl['cluster_'+str(i)][row]\n",
    "        if current_value > third_highest_value :\n",
    "            if current_value > second_highest_value:\n",
    "                if current_value > highest_value:\n",
    "                    third_highest_i = second_highest_i\n",
    "                    second_highest_i = highest_i\n",
    "                    highest_i = i\n",
    "                    \n",
    "                    third_highest_value = second_highest_value\n",
    "                    second_highest_value = highest_value\n",
    "                    highest_value = current_value\n",
    "                else:\n",
    "                    third_highest_i = second_highest_i\n",
    "                    second_highest_i = i\n",
    "                    \n",
    "                    third_highest_value = second_highest_value\n",
    "                    second_highest_value = current_value\n",
    "            else:\n",
    "                third_highest_value = current_value\n",
    "                \n",
    "                third_highest_i = i\n",
    "    \n",
    "    origins_grouped_ovrl['most_visited_cluster'][row] = highest_i\n",
    "    origins_grouped_ovrl['second_most_visited'][row] = second_highest_i\n",
    "    origins_grouped_ovrl['third_most_visited'][row] = third_highest_i\n",
    "\"\"\"\n",
    "origins_grouped_ovrl.nlargest(30, 'review_count' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_data = origins_grouped_ovrl.loc[origins_grouped_ovrl.index != '-'].sort_values('Average Daily Rate (USD)', ascending=False)\n",
    "\n",
    "trace1 = {\n",
    "  \"x\": list(trace_data.index),\n",
    "  \"y\": list(trace_data['Average Daily Rate (USD)']), \n",
    "  \"marker\": {\"color\": \"rgb(151, 160, 160)\"}, \n",
    "  \"name\": \"Average Daily Rate (USD)\", \n",
    "  \"type\": \"bar\", \n",
    "  \"uid\": \"247a10\", \n",
    "  \"xsrc\": \"qiweihan:653:a556e9\", \n",
    "  \"ysrc\": \"qiweihan:653:582f23\",\n",
    "  \"text\": [\"# of reviews: \"+str(rate) for rate in trace_data['review_count']]\n",
    "}\n",
    "data = go.Data([trace1])\n",
    "layout = {\n",
    "  \"autosize\": True, \n",
    "  \"dragmode\": \"lasso\", \n",
    "  \"hovermode\": \"x\", \n",
    "  \"title\": \"Average Daily Rates for each origin\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": True, \n",
    "    \"range\": [-0.5, 56.5], \n",
    "    \"showspikes\": False, \n",
    "    \"title\": \"\", \n",
    "    \"type\": \"category\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "    \"range\": [0, 106.383682389], \n",
    "    \"showspikes\": False, \n",
    "    \"title\": \"Avg Daily Rate\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'ADR_origins.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldf = locations[['Property ID', 'cluster','Review Date', \n",
    "                 'Occupancy Rate','Number of Reservations', 'Reservation Days']]\n",
    "\n",
    "tdf = tourists[['Property ID', 'Review Date', 'final_country_parse' ]]\n",
    "\n",
    "def estimated_avg_stay(row):\n",
    "    try:\n",
    "        return row['Reservation Days']/row['Number of Reservations']\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "ldf['estimated_avg_stay'] = ldf.apply(lambda x: estimated_avg_stay(x), axis=1)\n",
    "\n",
    "# formatting data for length of stay per country of origin\n",
    "stay_estimate_origin = pd.merge(tdf, ldf, how='left', on=['Property ID','Review Date']) \\\n",
    "                        [[ 'final_country_parse', 'Review Date', 'estimated_avg_stay' ]]\n",
    "\n",
    "stay_estimate_origin_grouped = stay_estimate_origin[[ 'final_country_parse','estimated_avg_stay' ]] \\\n",
    "                                .groupby(['final_country_parse']).mean().reset_index()\n",
    "\n",
    "review_count = tdf.groupby('final_country_parse').size().to_frame(name='review_count').reset_index()\n",
    "\n",
    "stay_estimate_origin_grouped = pd.merge(stay_estimate_origin_grouped, review_count, how='left' , on='final_country_parse' ) \\\n",
    "                                .nlargest(50,'review_count')\n",
    "\n",
    "stay_estimate_origin_grouped = stay_estimate_origin_grouped.sort_values('estimated_avg_stay', ascending=False)\n",
    "\n",
    "# formatting data for length of stay per region\n",
    "stay_estimate_country = pd.merge(tdf, ldf, how='left', on=['Property ID','Review Date']) \\\n",
    "                        [[ 'cluster', 'Review Date', 'estimated_avg_stay' ]]\n",
    "\n",
    "stay_estimate_country_grouped = stay_estimate_country[['cluster', 'estimated_avg_stay']].groupby('cluster').mean()\\\n",
    "                                .reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_data_2 = stay_estimate_origin_grouped[stay_estimate_origin_grouped['final_country_parse'] != '-']\n",
    "\n",
    "trace1 = {\n",
    "  \"x\": list(trace_data_2['final_country_parse']),\n",
    "  \"y\": list(trace_data_2['estimated_avg_stay']), \n",
    "  \"marker\": {\"color\": \"rgb(151, 160, 160)\"}, \n",
    "  \"name\": \"Estimated Average Stay\", \n",
    "  \"type\": \"bar\", \n",
    "  \"uid\": \"247a10\", \n",
    "  \"xsrc\": \"qiweihan:653:a556e9\", \n",
    "  \"ysrc\": \"qiweihan:653:582f23\"\n",
    "}\n",
    "data = go.Data([trace1])\n",
    "layout = {\n",
    "  \"autosize\": True, \n",
    "  \"dragmode\": \"lasso\", \n",
    "  \"hovermode\": \"x\", \n",
    "  \"title\": \"Average Length of stay for each origin\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": True, \n",
    "    \"range\": [-0.5, 56.5], \n",
    "    \"showspikes\": False, \n",
    "    \"title\": \"\", \n",
    "    \"type\": \"category\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "    \"range\": [0, 106.383682389], \n",
    "    \"showspikes\": False, \n",
    "    \"title\": \"Estimated Average Stay (days)\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'estimated_lengths_of_stay_origins.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_estimate_country_grouped = stay_estimate_country_grouped.sort_values('estimated_avg_stay', ascending=False)\n",
    "\n",
    "trace1 = {\n",
    "  \"x\": list(stay_estimate_country_grouped['cluster']),\n",
    "  \"y\": list(stay_estimate_country_grouped['estimated_avg_stay']), \n",
    "  \"marker\": {\"color\": \"rgb(151, 160, 160)\"}, \n",
    "  \"name\": \"Estimated Average Stay\", \n",
    "  \"type\": \"bar\", \n",
    "  \"uid\": \"247a10\", \n",
    "  \"xsrc\": \"qiweihan:653:a556e9\", \n",
    "  \"ysrc\": \"qiweihan:653:582f23\"\n",
    "}\n",
    "data = go.Data([trace1])\n",
    "layout = {\n",
    "  \"autosize\": True, \n",
    "  \"dragmode\": \"lasso\", \n",
    "  \"hovermode\": \"x\", \n",
    "  \"title\": \"Average Length of stay for each region\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": True, \n",
    "    \"range\": [-0.5, 56.5], \n",
    "    \"showspikes\": False, \n",
    "    \"tickangle\":45,\n",
    "    \"title\": \"\", \n",
    "    \"type\": \"category\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "    \"range\": [0, 106.383682389], \n",
    "    \"showspikes\": False, \n",
    "    \"title\": \"Estimated Average Stay (days)\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'estimated_lengths_of_stay_regions.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "price_variation = locations[['cluster', 'Reporting Month', 'Reservation Days', 'Revenue (USD)', 'ADR (USD)']]\n",
    "#price_variation['estimated_daily_revenue'] = price_variation['Revenue (USD)'] / price_variation['Reservation Days']\n",
    "\n",
    "price_variation = price_variation.groupby(['cluster', 'Reporting Month']).mean().reset_index()\n",
    "price_variation_2 = locations[['cluster', 'Reporting Month', 'ADR (USD)', 'Number of Reservations']]\n",
    "price_variation_2['reservations_adr_multiplied'] = price_variation_2['ADR (USD)']*price_variation_2['Number of Reservations']\n",
    "price_variation_2['sum_of_reservations'] = price_variation_2['Number of Reservations']\n",
    "price_variation_2 = price_variation_2.groupby(['cluster', 'Reporting Month']).sum().reset_index()\n",
    "price_variation_2['weighted_adr'] = price_variation_2['reservations_adr_multiplied']/price_variation_2['sum_of_reservations']\n",
    "\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:\n",
    "    filter_condition = price_variation['cluster'] == cluster\n",
    "    trace = {\n",
    "        \"x\": price_variation_2[filter_condition]['Reporting Month'], \n",
    "        \"y\": price_variation_2[filter_condition]['weighted_adr'], \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Weighted Average Daily Rate per region\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2015-12-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [0, 220], \n",
    "    \"title\": \"Weighted ADR\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'estimated_adr_per_region.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": regions_grouped[regions_grouped['cluster'] == cluster]['Review Date'], \n",
    "        \"y\": regions_grouped[regions_grouped['cluster'] == cluster]['Avg Occupancy Rate'], \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Average Occupancy Rate per region\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Avg Occupancy Rate\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'occupancy_rate_per_region.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "origins_per_cluster = tourists[['Review Date', 'final_country_parse']]\n",
    "origins_per_cluster_1 = origins_per_cluster.groupby(by=['final_country_parse']).size().to_frame(name='count').reset_index()\n",
    "origins_per_cluster_1 = origins_per_cluster_1[origins_per_cluster_1['final_country_parse'] != '-'].nlargest(14, 'count')\n",
    "countries = list(origins_per_cluster_1['final_country_parse'])\n",
    "\n",
    "def top_countries_filter(country):\n",
    "    if country not in countries:\n",
    "        country = 'others'\n",
    "    return country\n",
    "\n",
    "origins_per_cluster_2 = origins_per_cluster\n",
    "origins_per_cluster_2 = origins_per_cluster_2[origins_per_cluster_2['final_country_parse'] != '-']\n",
    "origins_per_cluster_2['final_country_parse'] = origins_per_cluster_2['final_country_parse'].apply(top_countries_filter) \n",
    "origins_per_cluster_2 = origins_per_cluster_2.groupby(by=['Review Date', 'final_country_parse']).size().to_frame(name='count')\n",
    "origins_per_cluster_2 = origins_per_cluster_2.reset_index()\n",
    "\n",
    "origins_per_cluster_2\n",
    "\n",
    "trace_list = []\n",
    "countries.append('others')\n",
    "for country in countries:\n",
    "    filter__ = origins_per_cluster_2['final_country_parse'] == country\n",
    "    trace = {\n",
    "        \"x\": origins_per_cluster_2[filter__]['Review Date'], \n",
    "        \"y\": origins_per_cluster_2[filter__]['count'], \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": country,\n",
    "        \"text\": country,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Count of booking reviews per Nationality\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Count\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'count_of_reviews_per_nationality.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the tables we will use\n",
    "rev = tables_filtered['reviews'][['User ID', 'Property ID', 'final_country_parse', 'Review Date']]\n",
    "prop = pd.merge(tables_filtered['properties'][[ 'Property ID', 'Average Daily Rate (USD)', 'Listing Type', 'Bedrooms', 'cluster' ]], val[['Property ID', 'val_cluster']], on='Property ID', how='left')\n",
    "day = tables_filtered['daily'][tables_filtered['daily']['Status'] == 'R'][['Property ID', 'Booked Date', 'Date', 'Price (USD)']]\n",
    "mon = tables_filtered['monthly'][[ 'Property ID', 'Reporting Month', 'Occupancy Rate', 'ADR (USD)', 'Number of Reservations', 'Revenue (USD)', 'Reservation Days' ]]\n",
    "\n",
    "# Merge tables into 2\n",
    "tourists = pd.merge(rev, prop, how= 'left', on= 'Property ID')\n",
    "locations = pd.merge(mon, prop, how = 'left', on= 'Property ID')\n",
    "\n",
    "# Format columns\n",
    "tourists['Review Date'] = pd.to_datetime(tourists['Review Date'])\n",
    "locations['Reporting Month'] = pd.to_datetime(locations['Reporting Month'])\n",
    "locations['Review Date'] = locations['Reporting Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldfv = locations[[ 'val_cluster',\n",
    "                'Occupancy Rate', \n",
    "                'ADR (USD)',\n",
    "                'Number of Reservations', \n",
    "                'Revenue (USD)', \n",
    "                'Reservation Days',\n",
    "                'Listing Type', \n",
    "                'Bedrooms']]\n",
    "\n",
    "\n",
    "value_profiling = ldfv.groupby('val_cluster').mean().reset_index()\n",
    "value_frequency = prop.groupby('val_cluster').size().to_frame(name='frequency').reset_index()\n",
    "value_profiling = pd.merge(value_frequency, value_profiling, on='val_cluster')\n",
    "#value_profiling\n",
    "\n",
    "def round_decimals(value):\n",
    "    return np.round(value, decimals=3)\n",
    "\n",
    "# configure table viz\n",
    "trace = go.Table(\n",
    "    header=dict(values=value_profiling.columns, fill = dict(color='#ea7870'), align = ['left'] * 5),\n",
    "    cells=dict(values=[value_profiling[column].apply(round_decimals) for column in value_profiling.columns], fill= dict(color='#efd9d7'), \n",
    "               align = ['left'] * 5))\n",
    "plotly.offline.plot([trace], filename=viz_out+'value_clusters_profiling.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot([trace], show_link=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trace_list = []\n",
    "val_cluster_list = list(val.sort_values('val_cluster')['val_cluster'].unique())\n",
    "for cluster in val_cluster_list:\n",
    "    cluster_select = val['val_cluster'] == cluster\n",
    "    trace = go.Scattermapbox(\n",
    "                lat = val[cluster_select]['Latitude'],\n",
    "                lon = val[cluster_select]['Longitude'],\n",
    "                mode='markers',\n",
    "                name=\"Cluster \"+str(cluster),\n",
    "                marker=go.Marker(size= 2,\n",
    "                                 color=cluster ),\n",
    "                text = val[cluster_select]['val_cluster']\n",
    "                )\n",
    "    trace_list.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Location of listings',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_token,\n",
    "        bearing=0,\n",
    "        style='dark',\n",
    "        center=dict(\n",
    "            lat=39.64,\n",
    "            lon=-7.95,\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=4.2\n",
    "    ),\n",
    ")\n",
    "\n",
    "super_sub_map = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(super_sub_map, filename=viz_out+'value_clustering.html', show_link=False, auto_open=False)\n",
    "\n",
    "plotly.offline.iplot(super_sub_map, show_link=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overall_indexed = indexed[['date', 'property_count', 'bedroom_count']].groupby('date').sum().reset_index()\n",
    "\n",
    "\n",
    "overall_revpar_table = overall_grouped[['Review Date', 'Sum of Reservations', 'projected_revenue']]\n",
    "overall_revpar_table['date'] = overall_revpar_table['Review Date']\n",
    "overall_revpar_table = overall_revpar_table[['date', 'Sum of Reservations', 'projected_revenue']]\n",
    "\n",
    "overall_revpar_table = pd.merge(overall_revpar_table, overall_indexed, on=['date'], how='left')\n",
    "\n",
    "overall_revpar_table.dropna()\n",
    "\n",
    "\n",
    "overall_revpar_table['revpar'] = overall_revpar_table['projected_revenue']/overall_revpar_table['bedroom_count']/30\n",
    "supply_demand_table['revpar'] = supply_demand_table['projected_revenue']/supply_demand_table['bedroom_count']/30\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": supply_demand_table[supply_demand_table['cluster'] == cluster]['date'], \n",
    "        \"y\": supply_demand_table[supply_demand_table['cluster'] == cluster]['revpar'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "# total revpar without region distinction\n",
    "trace = {\n",
    "        \"x\": overall_revpar_table['date'], \n",
    "        \"y\": overall_revpar_table['revpar'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": 'Total',\n",
    "        \"text\": 'Total',\n",
    "        }\n",
    "trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"RevPAR\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2018-01-15 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"RevPAR\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'Revpar_per_cluster_and_date.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly RevPAR\n",
    "\n",
    "supply_demand_table['Year'] = supply_demand_table['Review Date'].dt.year\n",
    "\n",
    "yearly_revpar_overall = supply_demand_table[['Year', 'projected_revenue', 'bedroom_count']].groupby('Year').sum().reset_index()\n",
    "yearly_revpar_overall['revpar'] = yearly_revpar_overall['projected_revenue']/yearly_revpar_overall['bedroom_count']/30\n",
    "\n",
    "yearly_revpar_clustered = supply_demand_table[['cluster','Year', 'projected_revenue', 'bedroom_count']] \\\n",
    "        .groupby(['Year','cluster']).sum().reset_index()\n",
    "\n",
    "yearly_revpar_clustered['revpar'] = yearly_revpar_clustered['projected_revenue']/yearly_revpar_clustered['bedroom_count']/30\n",
    "\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": yearly_revpar_clustered[yearly_revpar_clustered['cluster'] == cluster]['Year'].astype('int'), \n",
    "        \"y\": yearly_revpar_clustered[yearly_revpar_clustered['cluster'] == cluster]['revpar'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "# total revpar without region distinction\n",
    "trace = {\n",
    "        \"x\": yearly_revpar_overall['Year'].astype('int'), \n",
    "        \"y\": yearly_revpar_overall['revpar'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": 'Total',\n",
    "        \"text\": 'Total',\n",
    "        }\n",
    "trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"RevPAR\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": True, \n",
    "    #\"range\": [\"2014-08-31 15:42:21.1765\", \"2018-01-15 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    #\"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"RevPAR\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'yearly_revpar.html', show_link=False, auto_open=True)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yearly_occupancy_rate = locations[locations['Number of Reservations'] > 0][['Review Date', 'cluster', 'Occupancy Rate']]\n",
    "yearly_occupancy_rate['Year'] = yearly_occupancy_rate['Review Date'].dt.year\n",
    "\n",
    "yearly_occupancy_rate = yearly_occupancy_rate[['Year', 'cluster', 'Occupancy Rate']] \\\n",
    "                                .groupby(['Year', 'cluster']).mean().reset_index()\n",
    "\n",
    "yearly_occupancy_rate_overall = locations[['Review Date', 'cluster', 'Occupancy Rate']]\n",
    "yearly_occupancy_rate_overall['Year'] = yearly_occupancy_rate_overall['Review Date'].dt.year\n",
    "\n",
    "yearly_occupancy_rate_overall = yearly_occupancy_rate_overall[['Year', 'cluster', 'Occupancy Rate']] \\\n",
    "                                .groupby(['Year']).mean().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": yearly_occupancy_rate[yearly_occupancy_rate['cluster'] == cluster]['Year'].astype('int'), \n",
    "        \"y\": yearly_occupancy_rate[yearly_occupancy_rate['cluster'] == cluster]['Occupancy Rate'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "# total revpar without region distinction\n",
    "trace = {\n",
    "        \"x\": yearly_occupancy_rate_overall['Year'].astype('int'), \n",
    "        \"y\": yearly_occupancy_rate_overall['Occupancy Rate'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": 'Total',\n",
    "        \"text\": 'Total',\n",
    "        }\n",
    "trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Occupancy Rate\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": True, \n",
    "    #\"range\": [\"2014-08-31 15:42:21.1765\", \"2018-01-15 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    #\"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"RevPAR\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'yearly_occupancy_rate.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supply_demand_table['Year'] = supply_demand_table['Review Date'].dt.year\n",
    "bedrooms_year = supply_demand_table[['cluster', 'Year', 'bedroom_count', 'property_count']].groupby(['cluster', 'Year']).max().reset_index()\n",
    "bedrooms_year_total = bedrooms_year.groupby('Year').sum().reset_index()\n",
    "bedrooms_year_total\n",
    "\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": bedrooms_year[bedrooms_year['cluster'] == cluster]['Year'].astype('int'), \n",
    "        \"y\": bedrooms_year[bedrooms_year['cluster'] == cluster]['bedroom_count'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "trace = {\n",
    "        \"x\": bedrooms_year_total['Year'], \n",
    "        \"y\": bedrooms_year_total['bedroom_count'],\n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": 'Total',\n",
    "        \"text\": 'Total',\n",
    "        }\n",
    "trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Bedroom Supply\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": True, \n",
    "    #\"range\": [\"2014-08-31 15:42:21.1765\", \"2018-01-15 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    #\"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Bedroom count\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'bedroom_count.html', show_link=False, auto_open=True)\n",
    "plotly.offline.iplot(fig, show_link=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regions_grouped['year'] = regions_grouped['Review Date'].dt.year\n",
    "\n",
    "regions_grouped_yearly = regions_grouped[['cluster', 'year', 'projected_revenue']].groupby(['cluster', 'year']).sum().reset_index()\n",
    "\n",
    "trace_list = []\n",
    "for cluster in cluster_nums:    \n",
    "    trace = {\n",
    "        \"x\": regions_grouped_yearly[regions_grouped_yearly['cluster'] == cluster]['year'], \n",
    "        \"y\": regions_grouped_yearly[regions_grouped_yearly['cluster'] == cluster]['projected_revenue'], \n",
    "        \"mode\": \"lines\", \n",
    "        \"name\": cluster,\n",
    "        \"text\": cluster,\n",
    "        }\n",
    "    trace_list.append(trace)\n",
    "\n",
    "data = go.Data(trace_list)\n",
    "layout = {\n",
    "  \"hovermode\": \"closest\", \n",
    "  \"title\": \"Revenue per cluster\", \n",
    "  \"xaxis\": {\n",
    "    \"autorange\": False, \n",
    "    \"range\": [\"2014-08-31 15:42:21.1765\", \"2017-12-04 18:52:56.4706\"], \n",
    "    \"title\": \"Date\", \n",
    "    \"type\": \"date\"\n",
    "  }, \n",
    "  \"yaxis\": {\n",
    "    \"autorange\": True, \n",
    "#    \"range\": [-5791.0, 110029], \n",
    "    \"title\": \"Sum of Revenue\", \n",
    "    \"type\": \"linear\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename=viz_out+'yearly_revenue.html', show_link=False, auto_open=False)\n",
    "plotly.offline.iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________\n",
    "## Draft Experiments Below\n",
    "_______________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(reviews))\n",
    "print(len(tables_filtered['reviews']))\n",
    "print(len(tables_filtered['reviews'].drop_duplicates()))\n",
    "print(len(tables['reviews']))\n",
    "print(len(tables['reviews'].drop_duplicates()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rev_df[rev_df['final_country_parse'] == 'British Indian Ocean Territory']#[['Country', 'City','final_country_parse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rev_df = reviews_pre[[ 'Property ID', 'User ID', 'Review Date', 'Member Since', 'First Name', \n",
    "                   'Country', 'State', 'City', 'Description', 'School', 'Work', \n",
    "                   'country_after_parse', 'country_from_city_parse', 'final_country_parse' ]]\n",
    "\n",
    "\n",
    "\n",
    "def correction(row):\n",
    "    if row['Country'] == 'DE' or row['Country'] == 'GERMANY':\n",
    "        return 'Germany'\n",
    "    elif row['Country'] == 'SOUTH AFRICA':\n",
    "        return 'South Africa'\n",
    "    elif row['Country'] == 'FR':\n",
    "        return 'France'\n",
    "    elif row['Country'] == 'NETHERLANDS':\n",
    "        return 'Netherlands'\n",
    "    elif row['Country'] == 'UNITED STATES':\n",
    "        return 'United States'\n",
    "    elif row['Country'] == 'PT':\n",
    "        return 'Portugal'\n",
    "    elif row['Country'] == 'UNITED ARAB EMIRATES':\n",
    "        return 'United Arab Emirates'\n",
    "    elif row['Country'] == 'AU':\n",
    "        return 'Australia'\n",
    "    elif row['Country'] == 'GB':\n",
    "        return 'United Kingdom'\n",
    "    elif row['Country'] == 'BR':\n",
    "        return 'Brazil'\n",
    "    elif row['Country'] == 'INDIA':\n",
    "        return 'India'\n",
    "    else:\n",
    "        return row['final_country_parse']\n",
    "    \n",
    "\n",
    "\n",
    "rev_df['final_country_parse'] = rev_df.apply(lambda x: correction(x), axis=1)\n",
    "rev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "locations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_revpar_overall['projected_revenue'] / yearly_revpar_overall['bedroom_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inertia_table.plot.line()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for cluster in cluster_nums:    \n",
    "#    print(regions_grouped[regions_grouped['cluster'] == cluster])\n",
    "    \n",
    "#regions_grouped['cluster'].dtype\n",
    "\n",
    "cluster_nums[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_pre = pd.read_csv(preprocess_dir+'Portugal_Review_Standardized.csv', error_bad_lines=False)\n",
    "len(reviews) # 1204312\n",
    "len(reviews_pre) # 1204222\n",
    "#tourists.sort_values('cluster')['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables_filtered['properties'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_filtered['properties'][['Annual Revenue LTM (USD)']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(reviews_lisbon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#properties['Bedrooms'].sum() # 209891.0\n",
    "\n",
    "supply_demand_table.keys()\n",
    "supply_demand_table['Year'] = supply_demand_table['Review Date'].dt.year\n",
    "bedrooms_year = supply_demand_table[['cluster', 'Year', 'bedroom_count', 'property_count']].groupby(['cluster', 'Year']).max().reset_index()\n",
    "bedrooms_year_total = bedrooms_year.groupby('Year').sum()\n",
    "bedrooms_year_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
